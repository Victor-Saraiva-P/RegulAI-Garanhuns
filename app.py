import streamlit as st
from config import *
from db import load_or_fetch_documents
from embeddings import create_embeddings, split_documents, create_or_load_vectorstore
from llm import init_llm, rag_search

# Configura a pÃ¡gina do Streamlit
st.set_page_config(
    page_title="RegulAI - Chatbot de Leis Municipais de Garanhuns",
    page_icon="ğŸ“œ",
    layout="centered"
)

st.title("ğŸ“œ RegulAI - Chatbot de Leis Municipais de Garanhuns ğŸ›ï¸")
st.write("Pergunte sobre leis municipais e receba respostas baseadas nos textos legais!")

# Carrega documentos (do cache ou do MongoDB, dependendo da disponibilidade)
raw_documents = load_or_fetch_documents()
if not raw_documents:
    st.error("Nenhum documento vÃ¡lido encontrado. Encerrando execuÃ§Ã£o.")
    st.stop()

# Cria ou carrega modelo de embeddings
embeddings = create_embeddings()

# Divide documentos em pedaÃ§os (chunks)
split_docs = split_documents(raw_documents)
if not split_docs:
    st.error("Nenhum pedaÃ§o de documento encontrado para FAISS.")
    st.stop()

# Cria ou carrega o Ã­ndice vetorial (FAISS)
vector_store = create_or_load_vectorstore(split_docs, embeddings)

# Inicializa a LLM (Groq)
llm = init_llm()

st.subheader("ğŸ’¬ FaÃ§a sua pergunta:")
user_input = st.text_input("Digite sua pergunta:")

# Se o usuÃ¡rio inserir uma pergunta, faz a busca via RAG e exibe a resposta
if user_input:
    with st.spinner("Buscando resposta..."):
        response = rag_search(user_input, vector_store, llm)
        st.text_area("Resposta:", response, height=200)
