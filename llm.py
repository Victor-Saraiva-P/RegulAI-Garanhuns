import os
import re
from dotenv import load_dotenv
from langchain_groq import ChatGroq

load_dotenv()
groqModel = os.environ.get('GROQ_MODEL')

def init_llm():
    """
    Inicializa a LLM (ChatGroq) com o modelo definido em .env.
    Retorna o objeto de modelo configurado.
    """
    print("Configurando LLM Groq...\n")
    llm = ChatGroq(model=groqModel)
    print("LLM configurado!")
    return llm

def rag_search(query: str, vector_store, llm):
    """
    Faz uma busca RAG (Retrieval-Augmented Generation) utilizando:
      - busca exata por n√∫mero de lei, se presente na query;
      - caso contr√°rio, busca vetorial no FAISS.
    Em seguida, constr√≥i um prompt com contexto e invoca a LLM.
    Retorna o texto de resposta gerado.
    """
    # Regex para identificar n√∫mero de lei no formato 'X.XXX/AAAA' ou 'XXXX/AAAA'
    lei_match = re.search(r"\b(\d{1,5}(?:\.\d{1,5})?/\d{4})\b", query)
    if lei_match:
        raw_numero = lei_match.group(1)
        # Remove pontos para corresponder ao que est√° salvo nos metadados
        numero_lei = raw_numero.replace(".", "")
        print(f"Usu√°rio perguntou especificamente pela lei: {numero_lei}")

        # Filtra chunks cujo metadata tenha 'numero_lei' igual ao que foi capturado
        matched_chunks = [
            doc for doc in vector_store.docstore._dict.values()
            if doc.metadata.get("numero_lei") == numero_lei
        ]

        if matched_chunks:
            print(f"Encontramos {len(matched_chunks)} chunks com esta lei.")
            relevant_docs = matched_chunks
        else:
            print("N√£o encontramos um match exato. Indo para busca vetorial...")
            relevant_docs = vector_store.similarity_search(query, k=5)
    else:
        relevant_docs = vector_store.similarity_search(query, k=5)

    print(f"Documentos relevantes: {len(relevant_docs)}")
    for i, doc in enumerate(relevant_docs):
        metadata = doc.metadata
        print(f"üìú Doc {i+1} - Lei: {metadata.get('numero_lei')}")
        print(f"   Trecho: {doc.page_content[:200]}...\n")

    context = "\n".join([doc.page_content for doc in relevant_docs])

    # Instru√ß√µes de resposta para a LLM
    prompt = (
        f"Voc√™ √© um especialista jur√≠dico encarregado de interpretar leis municipais de Garanhuns. "
        f"Seu objetivo √© fornecer respostas claras, precisas e diretas para perguntas sobre cada lei. "
        f"Sempre baseie suas respostas exclusivamente no texto da lei fornecida, sem suposi√ß√µes externas. "
        f"Se uma pergunta mencionar um n√∫mero de lei, verifique se ele est√° presente no texto antes de responder. "
        f"\n\nRegras para resposta:"
        f"\n- Se houver uma data mencionada, forne√ßa exatamente essa data sem alterar o formato."
        f"\n- Se houver um nome pr√≥prio, transcreva-o exatamente como aparece no texto da lei."
        f"\n- Se houver um valor financeiro, forne√ßa exatamente o valor sem reescrev√™-lo ou arredond√°-lo."
        f"\n- Se a informa√ß√£o n√£o estiver no texto, responda explicitamente que o dado n√£o foi encontrado."
        f"\n- Mantenha a resposta objetiva e sem repeti√ß√µes desnecess√°rias."
        f"\n- Se a pergunta tiver um formato Existe alguma lei? Sempre cite a lei."
        f"\n\n{context}\n\nPergunta: {query}\nResposta:"
    )

    # Chamada ao modelo
    try:
        print("Enviando prompt para LLM...")
        response_ai_message = llm.invoke(prompt)
        response_text = response_ai_message.content
        print("Resposta gerada!")
    except Exception as e:
        response_text = f"Erro ao invocar LLM: {e}"

    return response_text

